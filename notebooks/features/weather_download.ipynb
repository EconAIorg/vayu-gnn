{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "from vayu_gnn.dbx.dbx_config import dbx_helper, DropboxHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add extra nodes for neighboring weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'nodes.pickle' successfully uploaded to Dropbox path: '/input/clean/node_locations/Patna/nodes.pickle'\n",
      "Shapefile written to temporary directory: /var/folders/w2/spn01gpx39d_btp2kmjt17jh0000gn/T/tmpamjruw37\n",
      "File 'nodes.shp' successfully uploaded to Dropbox at '/input/clean/node_locations/Patna/gdf/nodes.shp'\n",
      "File 'nodes.shx' successfully uploaded to Dropbox at '/input/clean/node_locations/Patna/gdf/nodes.shx'\n",
      "File 'nodes.dbf' successfully uploaded to Dropbox at '/input/clean/node_locations/Patna/gdf/nodes.dbf'\n",
      "File 'nodes.prj' successfully uploaded to Dropbox at '/input/clean/node_locations/Patna/gdf/nodes.prj'\n",
      "File 'nodes.cpg' successfully uploaded to Dropbox at '/input/clean/node_locations/Patna/gdf/nodes.cpg'\n"
     ]
    }
   ],
   "source": [
    "city = 'Patna'\n",
    "\n",
    "devices = dbx_helper.read_pickle(dbx_helper.clean_input_path, 'map_device_to_latlong', f'{city}_static.pickle')\n",
    "\n",
    "# Calculate the center (average latitude and longitude) from the existing devices\n",
    "center_lat = float(sum(device['lat'] for device in devices.values()) / len(devices))\n",
    "center_long = float(sum(device['long'] for device in devices.values()) / len(devices))\n",
    "\n",
    "half_side = 1\n",
    "offsets = np.linspace(-half_side, half_side, 9)  # 9 equally spaced points from -1 to 1\n",
    "\n",
    "extra_nodes = {}\n",
    "node_counter = 1\n",
    "\n",
    "# Top side: fixed latitude (center_lat + 1), longitude varies by offset\n",
    "for offset in offsets:\n",
    "    device_id = f\"extra_node_{node_counter}\"\n",
    "    lat_val = float(round(center_lat + half_side, 4))\n",
    "    long_val = float(round(center_long + offset, 4))\n",
    "    extra_nodes[device_id] = {'lat': lat_val, 'long': long_val}\n",
    "    node_counter += 1\n",
    "\n",
    "# Bottom side: fixed latitude (center_lat - 1), longitude varies by offset\n",
    "for offset in offsets:\n",
    "    device_id = f\"extra_node_{node_counter}\"\n",
    "    lat_val = float(round(center_lat - half_side, 4))\n",
    "    long_val = float(round(center_long + offset, 4))\n",
    "    extra_nodes[device_id] = {'lat': lat_val, 'long': long_val}\n",
    "    node_counter += 1\n",
    "\n",
    "# Left side: fixed longitude (center_long - 1), latitude varies by offset (excluding corners)\n",
    "for offset in offsets[1:-1]:\n",
    "    device_id = f\"extra_node_{node_counter}\"\n",
    "    lat_val = float(round(center_lat + offset, 4))\n",
    "    long_val = float(round(center_long - half_side, 4))\n",
    "    extra_nodes[device_id] = {'lat': lat_val, 'long': long_val}\n",
    "    node_counter += 1\n",
    "\n",
    "# Right side: fixed longitude (center_long + 1), latitude varies by offset (excluding corners)\n",
    "for offset in offsets[1:-1]:\n",
    "    device_id = f\"extra_node_{node_counter}\"\n",
    "    lat_val = float(round(center_lat + offset, 4))\n",
    "    long_val = float(round(center_long + half_side, 4))\n",
    "    extra_nodes[device_id] = {'lat': lat_val, 'long': long_val}\n",
    "    node_counter += 1\n",
    "\n",
    "combined_devices = {**devices, **extra_nodes}\n",
    "\n",
    "dbx_helper.write_pickle(combined_devices, dbx_helper.clean_input_path, f'node_locations/{city}', f'nodes.pickle')\n",
    "\n",
    "# Also save as a gdf\n",
    "records = []\n",
    "for device_id, coords in combined_devices.items():\n",
    "    records.append({\n",
    "        'device_id': device_id,\n",
    "        'lat': coords['lat'],\n",
    "        'long': coords['long']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['geometry'] = df.apply(lambda row: Point(row['long'], row['lat']), axis=1)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "dbx_helper.write_shp(gdf, dbx_helper.clean_input_path, f'node_locations/{city}/gdf', f'nodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch historical weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests_cache\n",
    "from openmeteo_requests import Client\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = Client(session=retry_session)\n",
    "\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "# Base params\n",
    "params = {\n",
    "    \"start_date\": \"2024-05-01\",\n",
    "    \"end_date\": \"2025-02-28\",\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"relative_humidity_2m\",\n",
    "        \"precipitation\",\n",
    "        \"soil_temperature_0_to_7cm\",\n",
    "        \"soil_moisture_0_to_7cm\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloud_cover\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for node_name, coords in combined_devices.items():\n",
    "    print(node_name)\n",
    "    \n",
    "    params[\"latitude\"] = coords[\"lat\"]\n",
    "    params[\"longitude\"] = coords[\"long\"]\n",
    "    \n",
    "    # Query the API (assuming each call returns a list with one response)\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "    \n",
    "    # Extract the API returned coordinates and metadata\n",
    "    api_lat = response.Latitude()\n",
    "    api_long = response.Longitude()\n",
    "    \n",
    "    # Process hourly data from the response\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_soil_temperature_0_to_7cm = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_soil_moisture_0_to_7cm = hourly.Variables(6).ValuesAsNumpy()\n",
    "    hourly_surface_pressure = hourly.Variables(7).ValuesAsNumpy()\n",
    "    hourly_cloud_cover = hourly.Variables(8).ValuesAsNumpy()\n",
    "    \n",
    "    # Create a date range based on the hourly metadata\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add all the requested weather variables to the data dictionary\n",
    "    hourly_data[\"temperature\"] = hourly_temperature_2m\n",
    "    hourly_data[\"wind_speed\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_direction\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"humidity\"] = hourly_relative_humidity_2m\n",
    "    hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "    hourly_data[\"soil_temperature\"] = hourly_soil_temperature_0_to_7cm\n",
    "    hourly_data[\"soil_moisture\"] = hourly_soil_moisture_0_to_7cm\n",
    "    hourly_data[\"pressure\"] = hourly_surface_pressure\n",
    "    hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "    \n",
    "    # Create a DataFrame for the current device\n",
    "    df = pd.DataFrame(data=hourly_data)\n",
    "    \n",
    "    # Add metadata columns from both the device dictionary and the API response\n",
    "    df[\"node\"] = node_name\n",
    "    df[\"device_lat\"] = coords[\"lat\"]\n",
    "    df[\"device_long\"] = coords[\"long\"]\n",
    "    df[\"om_lat\"] = api_lat\n",
    "    df[\"om_long\"] = api_long\n",
    "\n",
    "    # write to dropbox\n",
    "    dbx_helper.write_csv(df, dbx_helper.clean_input_path, f'node_level_weather_data/historical_weather/{city}', f\"{city}_{node_name}.csv\")\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Patna'\n",
    "\n",
    "base_path = dbx_helper.clean_input_path\n",
    "directory = f'node_level_weather_data/historical_weather/{city}'\n",
    "full_path = os.path.join(base_path, directory)\n",
    "\n",
    "files = dbx_helper.list_files_in_folder(full_path)\n",
    "dfs = [dbx_helper.read_csv(base_path, directory, file) for file in files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "dbx_helper.write_csv(combined_df, base_path, 'node_level_weather_data/historical_weather', f'{city}_historical_weather_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch historical weather forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = Client(session=retry_session)\n",
    "\n",
    "url = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\n",
    "# Base params\n",
    "params = {\n",
    "    \"start_date\": \"2024-05-01\",\n",
    "    \"end_date\": \"2025-02-28\",\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"relative_humidity_2m\",\n",
    "        \"precipitation\",\n",
    "        \"soil_temperature_0cm\",\n",
    "        \"soil_moisture_0_to_1cm\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloud_cover\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for node_name, coords in combined_devices.items():\n",
    "    print(node_name)\n",
    "    \n",
    "    params[\"latitude\"] = coords[\"lat\"]\n",
    "    params[\"longitude\"] = coords[\"long\"]\n",
    "    \n",
    "    # Query the API (assuming each call returns a list with one response)\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "    \n",
    "    # Extract the API returned coordinates and metadata\n",
    "    api_lat = response.Latitude()\n",
    "    api_long = response.Longitude()\n",
    "    \n",
    "    # Process hourly data from the response\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_soil_temperature_0cm = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_soil_moisture_0_to_1cm = hourly.Variables(6).ValuesAsNumpy()\n",
    "    hourly_surface_pressure = hourly.Variables(7).ValuesAsNumpy()\n",
    "    hourly_cloud_cover = hourly.Variables(8).ValuesAsNumpy()\n",
    "    \n",
    "    # Create a date range based on the hourly metadata\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add all the requested weather variables to the data dictionary\n",
    "    hourly_data[\"temperature\"] = hourly_temperature_2m\n",
    "    hourly_data[\"wind_speed\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_direction\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"humidity\"] = hourly_relative_humidity_2m\n",
    "    hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "    hourly_data[\"soil_temperature\"] = hourly_soil_temperature_0cm\n",
    "    hourly_data[\"soil_moisture\"] = hourly_soil_moisture_0_to_1cm\n",
    "    hourly_data[\"pressure\"] = hourly_surface_pressure\n",
    "    hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "    \n",
    "    # Create a DataFrame for the current device\n",
    "    df = pd.DataFrame(data=hourly_data)\n",
    "    \n",
    "    # Add metadata columns from both the device dictionary and the API response\n",
    "    df[\"node\"] = node_name\n",
    "    df[\"device_lat\"] = coords[\"lat\"]\n",
    "    df[\"device_long\"] = coords[\"long\"]\n",
    "    df[\"om_lat\"] = api_lat\n",
    "    df[\"om_long\"] = api_long\n",
    "\n",
    "    # write to dropbox\n",
    "    dbx_helper.write_csv(df, dbx_helper.clean_input_path, f'node_level_weather_data/historical_weather_forecasts/{city}', f\"{city}_{node_name}.csv\")\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder '/input/clean/node_level_weather_data/historical_weather_forecasts/Patna':\n",
      "Size of the CSV file: 58.64 MB\n",
      "File 'Patna_historical_weather_forecast_data.csv' successfully uploaded to Dropbox path: '/input/clean/node_level_weather_data/historical_weather_forecasts/Patna/Patna_historical_weather_forecast_data.csv'\n"
     ]
    }
   ],
   "source": [
    "city = 'Patna'\n",
    "\n",
    "base_path = dbx_helper.clean_input_path\n",
    "directory = f'node_level_weather_data/historical_weather_forecasts/{city}'\n",
    "full_path = os.path.join(base_path, directory)\n",
    "\n",
    "files = dbx_helper.list_files_in_folder(full_path)\n",
    "dfs = [dbx_helper.read_csv(base_path, directory, file) for file in files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "dbx_helper.write_csv(combined_df, base_path, 'node_level_weather_data/historical_weather_forecasts', f'{city}_historical_weather_forecast_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
